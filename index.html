<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MoreFusion: Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="assets/css/main.css">
  </head>
  <body>
    <div class="container-fluid">
      <div class="row">
        <div class="col-lg-8 offset-lg-2">

          <div class="text-center">
            <h1 class="mt-5"><i>More</i><b>Fusion</b></h1>
            <h4 class="mt-4">Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion</h4>
            <ul class="list-inline mt-4">
              <li class="list-inline-item"><a href="https://wkentaro.com" target="_blank">Kentaro Wada</a></li>
              <li class="list-inline-item ml-4">Edgar Sucar</li>
              <li class="list-inline-item ml-4"><a href="https://stepjam.github.io" target="_blank">Stephen James</a></li>
              <li class="list-inline-item ml-4">Daniel Lenton</li>
              <li class="list-inline-item ml-4"><a href="https://www.doc.ic.ac.uk/~ajd/" target="_blank">Andrew J. Davison</a></li>
              <li class="mt-2">
                <a href="https://www.imperial.ac.uk/dyson-robotics-lab/" target="_blank">Dyson Robotic Laboratory</a>,
                <a href="https://www.imperial.ac.uk/" target="_blank" class="ml-2">Imperial College London</a>
              </li>
            </ul>
            <ul class="list-inline mt-4">
              <li class="list-inline-item">
                <a href="https://drive.google.com/uc?id=1-D6_t1fcsUyWb-6O_6l0o_NW7nfZ0emg" target="_blank">Paper</a>
              </li>
              <li class="list-inline-item ml-4">
                <a href="https://github.com/wkentaro/morefusion.git" target="_blank">Code</a>
              </li>
              <li class="list-inline-item ml-4"><a href="assets/video/main.mp4" target="_blank">Video</a></li>
              <li class="list-inline-item ml-4">
                <a href="https://drive.google.com/uc?id=1mwa1gQy8ibuc3Gc-6lgvt64VFmH5CW4q" target="_blank">Dataset</a>
              </li>
            </ul>
          </div>

          <div class="row mt-4">
            <div class="col-lg-7 offset-lg-1">
              <p class="lead">
                Robots and other smart devices need efficient object-based scene
                representations from their on-board vision systems to reason about
                contact, physics and occlusion. Recognized precise object models
                will play an important role alongside non-parametric
                reconstructions of unrecognized structures.
              </p>
              <p class="lead">
                In this paper, we present a system,
                <u>MoreFusion</u>,
                which can estimate the accurate poses of multiple known objects in
                contact and occlusion from real-time, embodied multi-view vision.
                Our approach makes 3D object pose proposals from single RGB-D
                views, accumulates pose estimates and non-parametric occupancy
                information from multiple views as the camera moves, and performs
                joint optimization to estimate consistent, non-intersecting poses
                for multiple objects in contact.
              </p>
            </div>
            <div class="col-lg-3 text-center">
              <img src="assets/img/intro/scene.jpg" class="img-fluid" width="90%">
              <img src="assets/img/intro/pose.jpg" class="img-fluid mt-1" width="90%">
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Overview Video (with audio)</h4>
            <div align="center" class="row mt-4">
              <!-- <iframe width="640" height="360" src="#" allowfullscreen></iframe> -->
              <div class="col-lg-8 offset-lg-2">
                <video preload="metadata" width="100%" height="auto" controls>
                  <source src="assets/video/main.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Pipeline</h4>
            <p>
              We present a pipeline that achieves state-of-the-art results for 6D pose estimation of known objects, which
              (a) reconstructs a scene with volumetric fusion;
              (b) predicts object pose utilizing the volumetric reconstruction;
              (c) refines the predicted pose respecting surrounding geometry and pose predictions;
              (d) validates plural pose hypothesis to find a highly confident pose estimate.
            </p>
            <div class="pl-4 pr-4">
              <img src="assets/img/system.png" class="img-fluid">
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Surrounding Spatial Information</h4>
            <p>
              Each target object for pose prediction carries its own
              volumetric occupancy grid. The voxels that make up this grid can be
              in one of the following states: b) Space occupied by the object
              itself from the target object reconstruction; c) Space
              occupied by other objects; d) Free space identified by depth
              measurement. e) Unknown space unobserved by mapping
              because of occlusion and sensor range limit.
            </p>
            <div class="row">
              <div class="col-4 col-lg-2 offset-lg-1 text-center">
                <img src="assets/img/surrounding_information/scene.png" class="img-fluid">
                <p>(a) Scene</p>
              </div>
              <div class="col-4 col-lg-2 text-center">
                <img src="assets/img/surrounding_information/grid_self.png" class="img-fluid">
                <p>(b) Grid (self)</p>
              </div>
              <div class="col-4 col-lg-2 text-center">
                <img src="assets/img/surrounding_information/grid_other.png" class="img-fluid">
                <p>(c) Grid (other)</p>
              </div>
              <div class="col-4 col-lg-2 text-center">
                <img src="assets/img/surrounding_information/grid_free.png" class="img-fluid">
                <p>(d) Grid (free)</p>
              </div>
              <div class="col-4 col-lg-2 text-center">
                <img src="assets/img/surrounding_information/grid_unknown.png" class="img-fluid">
                <p>(e) Grid (unknown)</p>
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Pose Prediction Results</h4>
            <p>
              Our proposed model (MoreFusion) performs consistent pose prediction
              with surroundings, where the baseline (DenseFusion∗) and the
              variant without occupancy information
              (Morefusion<small>−occ</small>) fails.
            </p>
            <div class="table-responsive">
              <table class="table">
                <tr class="d-flex text-center">
                  <th class="col-md-2 offset-md-1">Scene</th>
                  <th class="col-md-2">DenseFusion*</th>
                  <th class="col-md-2">MoreFusion<small>-occ</small></th>
                  <th class="col-md-2">MoreFusion</th>
                  <th class="col-md-2">Ground Truth</th>
                </tr>
                <tr class="d-flex text-center">
                  <td class="col-md-2 offset-md-1"><img class="img-fluid" src="assets/img/pose_prediction/w_occ_rgb.jpg"></td>
                  <td class="col-md-2"><img class="img-fluid" src="assets/img/pose_prediction/pcd_res.jpg"></td>
                  <td class="col-md-2"><img class="img-fluid" src="assets/img/pose_prediction/wo_occ_res.jpg"></td>
                  <td class="col-md-2"><img class="img-fluid" src="assets/img/pose_prediction/w_occ_res.jpg"></td>
                  <td class="col-md-2"><img class="img-fluid" src="assets/img/pose_prediction/w_occ_gt.jpg"></td>
                </tr>
              </table>
            </div>
            <div class="row mt-4">
              <div class="col-lg-6">
                <p>
                  We evaluated the pose prediction in different datasets (YCB-Video, Cluttered-YCB),
                  and the result shows that the proposed model consistently gives better pose estimate.
                </p>
                <div class="table-responsive">
                  <table class="table table-bordered text-center">
                    <tr><th>Model</th><th>Dataset</th><th>ADD(-S)</th><th>ADD-S</th></tr>
                    <tr><th>DenseFusion*</th><th rowspan="2" style="vertical-align: middle;">YCB-Video</th><td>88.4</td><td>94.9</td></tr>
                    <tr><th>MoreFusion</th><td><u>91.0</u></td><td><u>95.7</u></td></tr>
                    <tr><th>DenseFusion*</th><th rowspan="2" style="vertical-align: middle;">Cluttered YCB</th><td>81.7</td><td>91.7</td></tr>
                    <tr><th>MoreFusion</th><td><u>83.4</u></td><td><u>92.3</u></td></tr>
                  </table>
                </div>
              </div>
              <div class="col-lg-6">
                <p>
                  As more and more occupancy information is available:
                  MF<small>-occ</small> (no occupancy) &lt; MF &lt; MF<small>+target-</small> (full grid of nontarget),
                  MF<small>+target- +bg</small> (full grid of background objects),
                  the model gives better pose estimate.
                </p>
                <div class="table-responsive">
                  <table class="table table-bordered text-center">
                    <tr><th>Model</th><th>ADD(-S)</th><th>ADD-S</th></tr>
                    <tr><th>MF<small>-occ</small></th><td>82.5</td><td>91.7</td></tr>
                    <tr><th>MoreFusion (MF)</th><td>83.4</td><td>92.3</td></tr>
                    <tr><th>MF<small>+target-</small></th><td>84.7</td><td>93.3</td></tr>
                    <tr><th>MF<small>+target- +bg</small></th><td>85.5</td><td>93.8</td></tr>
                  </table>
                </div>
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Pose Refinement Results</h4>
            <p>
              The proposed collision-based pose refinement, Iterative Collision Check (ICC),
              gives better convergence minimizing the intersections among objects,
              as well as aligning the model surface to the depth observation.
              For detailed refinement (right figure), it is better to combine ICC with Iterative Closest Point (ICP),
              to initially align model with ICC in volumetric resolution and then to refine the detail
              in point space with ICP.
            </p>
            <div class="row">
              <div class="col-lg-6">
                <video preload="metadata" width="100%" height="auto" controls>
                  <source src="assets/video/pose_refinement.mp4#t=10" type="video/mp4">
                </video>
              </div>
              <div class="col-lg-6">
                <img src="assets/img/refine.png" class="img-fluid">
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Scene Reconstruction</h4>
            <div class="row">
              <div class="col-lg-6">
                <p>Robotic Top-down Camera Sequence</p>
                <div id="reconstruction-robotic-top-down"></div>
                <video preload="metadata" width="100%" height="auto" controls>
                  <source src="assets/video/reconstruction1.mp4#t=29" type="video/mp4">
                </video>
              </div>
              <div class="col-lg-6">
                <p>Table-top Side-view Camera Sequence</p>
                <div id="reconstruction-table-top-side"></div>
                <video preload="metadata" width="100%" height="auto" controls>
                  <source src="assets/video/reconstruction2.mp4#t=15" type="video/mp4">
                </video>
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Robotic Pick-and-Place</h4>
            <div class="row">
              <div class="col-lg-6">
                <p>Pick-and-Place of the Target Object (Red Box) from a Pile</p>
                <video preload="metadata" width="100%" height="auto" controls>
                  <source src="assets/video/demonstration1.mp4#t=15" type="video/mp4">
                </video>
              </div>
              <div class="col-lg-6">
                <p>More Examples</p>
                <video preload="metadata" width="100%" height="auto" controls>
                  <source src="assets/video/demonstration2.mp4#t=15" type="video/mp4">
                </video>
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Bibtex</h4>
            <pre>
  @inproceedings{Wada:etal:CVPR2020,
    title={{MoreFusion}: Multi-object Reasoning for {6D} Pose Estimation from Volumetric Fusion},
    author={Kentaro Wada and Edgar Sucar and Stephen James and Daniel Lenton and Andrew J. Davison},
    booktitle={Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
    year={2020},
  }
            </pre>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1 mb-5">
            <h4>Contact</h4>
            <p>
              If you have any questions, please feel free to contact
              <a href="https://wkentaro.com" target="_blank">Kentaro Wada</a>.
            </p>
          </div>

        </div>
      </div>
    </div>

    <script src="assets/reconstruction/table-top-side/bundle.js" type="text/javascript"></script>
    <script src="assets/reconstruction/robotic-top-down/bundle.js" type="text/javascript"></script>
  </body>
</html>
